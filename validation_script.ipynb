{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dc4cb60-b4db-4050-b7e9-56c1cc6e238c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 21:56:10.856652: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-22 21:56:10.903602: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-22 21:56:11.639885: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "nltk.download('stopwords')\n",
    "english_stopwords = stopwords.words('english')\n",
    "hindi_stop_word_file = open(\"hindi_stopwords.txt\")\n",
    "hindi_stopwords = hindi_stop_word_file.read()\n",
    "hindi_stopwords = list(hindi_stopwords.split(\"\\n\"))\n",
    "hindi_stop_word_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71c1d98e-fcf1-462f-825d-5c1e4c908805",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_DATASET_PATH = \"./TrainingData/testing_dataset.csv\"\n",
    "\n",
    "# Listing categories, subcategories, and missing subcategories in the training dataset.\n",
    "# Creating a index map for categories and subcategories.\n",
    "# Categories in the training data. \n",
    "categories_list = [\n",
    "       'Online and Social Media Related Crime', 'Online Financial Fraud',\n",
    "       'Online Gambling  Betting',\n",
    "       'RapeGang Rape RGRSexually Abusive Content',\n",
    "       'Any Other Cyber Crime', 'Cyber Attack/ Dependent Crimes',\n",
    "       'Cryptocurrency Crime', 'Sexually Explicit Act',\n",
    "       'Sexually Obscene material',\n",
    "       'Hacking  Damage to computercomputer system etc',\n",
    "       'Cyber Terrorism',\n",
    "       'Child Pornography CPChild Sexual Abuse Material CSAM',\n",
    "       'Online Cyber Trafficking', 'Ransomware',\n",
    "       'Report Unlawful Content'\n",
    "]\n",
    "\n",
    "# Sub categories in the training data.\n",
    "sub_categories = [\n",
    "            'Cyber Bullying  Stalking  Sexting', 'Fraud CallVishing',\n",
    "           'Online Gambling  Betting', 'Online Job Fraud',\n",
    "           'UPI Related Frauds', 'Internet Banking Related Fraud',\n",
    "           'Other', 'Profile Hacking Identity Theft',\n",
    "           'DebitCredit Card FraudSim Swap Fraud', 'EWallet Related Fraud',\n",
    "           'Data Breach/Theft', 'Cheating by Impersonation',\n",
    "           'Denial of Service (DoS)/Distributed Denial of Service (DDOS) attacks',\n",
    "           'FakeImpersonating Profile', 'Cryptocurrency Fraud',\n",
    "           'Malware Attack', 'Business Email CompromiseEmail Takeover',\n",
    "           'Email Hacking', 'Hacking/Defacement',\n",
    "           'Unauthorised AccessData Breach', 'SQL Injection',\n",
    "           'Provocative Speech for unlawful acts', 'Ransomware Attack',\n",
    "           'Cyber Terrorism', 'Tampering with computer source documents',\n",
    "           'DematDepository Fraud', 'Online Trafficking',\n",
    "           'Online Matrimonial Fraud', 'Website DefacementHacking',\n",
    "           'Damage to computer computer systems etc', 'Impersonating Email',\n",
    "           'EMail Phishing', 'Ransomware', 'Intimidating Email',\n",
    "           'Against Interest of sovereignty or integrity of India'\n",
    "    ]\n",
    "additonal_subcat = [\n",
    "        'Sexually Explicit Act',\n",
    "        'Sexually Obscene material',\n",
    "        'Child Pornography CPChild Sexual Abuse Material CSAM',\n",
    "        'RapeGang Rape RGRSexually Abusive Content'\n",
    "    ]\n",
    "\n",
    "\n",
    "subcategory_list = sub_categories\n",
    "subcategory_list.extend(additonal_subcat)\n",
    "subcat_index_map = dict(\n",
    "        zip(\n",
    "            subcategory_list,\n",
    "            range(len(subcategory_list))\n",
    "        )\n",
    "    )\n",
    "\n",
    "cat_index_map = dict(\n",
    "    zip(\n",
    "        categories_list,\n",
    "        range(len(categories_list))\n",
    "        \n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d826b9d2-1597-4bed-b909-812913c73715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Total Number of samples :: 31229\n",
      "[*] Number of samples after removing empty, duplicate *Crime Additional Info*  :: 28785\n"
     ]
    }
   ],
   "source": [
    "# Data preparation : Dropping duplicate training and testing data with respect to crimeaditionalinfo column.\n",
    "# Removing rows with blank crimeaditionalinfo value.\n",
    "# Filling missing subcategories with categories\n",
    "# Ensure all text data is in string format\n",
    "# ***** We are only considering the sub categories, and sub categories in the training data. *******\n",
    "\n",
    "def get_info_and_labels(fpath : str, flag : bool = False):\n",
    "    df = pd.read_csv(fpath)\n",
    "    print(f\"[*] Total Number of samples :: {df.shape[0]}\")\n",
    "    if flag:\n",
    "        df = df.drop_duplicates(['crimeaditionalinfo'])\n",
    "    df = df[df['crimeaditionalinfo'].notnull()]\n",
    "    print(f\"[*] Number of samples after removing empty, duplicate *Crime Additional Info*  :: {df.shape[0]}\")\n",
    "\n",
    "    x, y_category, y_subcategory = [], [], []\n",
    "    for sub_cat in sub_categories:\n",
    "        sub_cat_df = df.query(f\"sub_category == '{sub_cat}'\")\n",
    "        x_content = sub_cat_df['crimeaditionalinfo'].values\n",
    "        y_sub_cat_values = sub_cat_df['sub_category'].values\n",
    "        y_cat_values = sub_cat_df['category'].values\n",
    "        x.extend(x_content)\n",
    "        y_subcategory.extend(y_sub_cat_values)\n",
    "        y_category.extend(y_cat_values)\n",
    "        \n",
    "        \n",
    "    for sub_cat in additonal_subcat:\n",
    "        sub_cat_df = df.query(f\"category == '{sub_cat}'\")\n",
    "        x_content = sub_cat_df['crimeaditionalinfo'].values\n",
    "        y_sub_cat_values = sub_cat_df['category'].values\n",
    "        y_cat_values = sub_cat_df['category'].values\n",
    "        x.extend(x_content)\n",
    "        y_subcategory.extend(y_sub_cat_values)\n",
    "        y_category.extend(y_cat_values)\n",
    "\n",
    "    return x, y_category, y_subcategory\n",
    "\n",
    "def filter_fxn(x):\n",
    "    if x == '' or len(x) <= 2 or len(x) > 15:\n",
    "        return False\n",
    "    if (x in english_stopwords) or (x in hindi_stopwords):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def transform_text(text):\n",
    "    text = text.lower()\n",
    "    valid_char = \"abcdefghijklmnopqrstuvwxyz\" + \" \"\n",
    "    escape_char_list = [\"\\r\", \"\\n\", \"\\b\", \"\\t\"]\n",
    "    for escape_char in escape_char_list:\n",
    "        text = text.replace(escape_char, \" \")\n",
    "        \n",
    "    for c in text:\n",
    "        if c not in valid_char:\n",
    "            text = text.replace(c, \"\")\n",
    "    text_list = text.split(\" \")\n",
    "    text_list = list(filter(filter_fxn, text_list))\n",
    "    return \" \".join(text_list)\n",
    "\n",
    "x_val, y_val_cat, y_val_subcat = get_info_and_labels(VALIDATION_DATASET_PATH, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca9c568e-7976-421a-a938-404dd3f428bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 21:56:14.257026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30925 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0\n",
      "2024-11-22 21:56:15.355676: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900/900 [==============================] - 2s 1ms/step\n",
      "Category classification Accuracy:  0.7275285778812411\n",
      "Category classification Precision:  0.7275285778812411\n",
      "Category classification Recall:  0.7275285778812411\n",
      "Category classification F1-score:  0.7275285778812413\n",
      "\n",
      "Subcategory classification Accuracy:  0.5044300059066745\n",
      "Subcategory classification Precision:  0.5044300059066745\n",
      "Subcategory classification Recall:  0.5044300059066745\n",
      "Subcategory classification F1-score 0.5044300059066745\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"CrimeReportTextClassification.keras\")\n",
    "from_disk = pickle.load(open(\"text_vector.pkl\", \"rb\"))\n",
    "text_vectorization = tf.keras.layers.TextVectorization.from_config(from_disk['config'])\n",
    "text_vectorization.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
    "text_vectorization.set_weights(from_disk['weights'])\n",
    "x_val = list(map(transform_text, x_val))\n",
    "y_val_cat_labels = []\n",
    "y_val_subcat_labels = []\n",
    "for sub_cat in y_val_subcat:\n",
    "    y_val_subcat_labels.append(subcat_index_map[sub_cat])\n",
    "for cat in y_val_cat:\n",
    "    y_val_cat_labels.append(cat_index_map[cat])\n",
    "y_val_cat = tf.keras.utils.to_categorical(y_val_cat_labels, num_classes = len(cat_index_map))\n",
    "y_val_subcat = tf.keras.utils.to_categorical(y_val_subcat_labels, num_classes = len(subcat_index_map))   \n",
    "x_val = text_vectorization(x_val)\n",
    "pred_cat, pred_subcat = model.predict(x_val)\n",
    "\n",
    "print(\"Category classification Accuracy: \", accuracy_score(np.argmax(y_val_cat, axis = 1), np.argmax(pred_cat, axis = 1)))\n",
    "print(\"Category classification Precision: \", precision_score(np.argmax(y_val_cat, axis = 1), np.argmax(pred_cat, axis = 1), average=\"micro\"))\n",
    "print(\"Category classification Recall: \", recall_score(np.argmax(y_val_cat, axis = 1), np.argmax(pred_cat, axis = 1), average=\"micro\"))\n",
    "print(\"Category classification F1-score: \", f1_score(np.argmax(y_val_cat, axis = 1), np.argmax(pred_cat, axis = 1), average=\"micro\"))\n",
    "print()\n",
    "print(\"Subcategory classification Accuracy: \", accuracy_score(np.argmax(y_val_subcat, axis = 1), np.argmax(pred_subcat, axis = 1)))\n",
    "print(\"Subcategory classification Precision: \", precision_score(np.argmax(y_val_subcat, axis = 1), np.argmax(pred_subcat, axis = 1), average=\"micro\"))\n",
    "print(\"Subcategory classification Recall: \", recall_score(np.argmax(y_val_subcat, axis = 1), np.argmax(pred_subcat, axis = 1), average=\"micro\"))\n",
    "print(\"Subcategory classification F1-score\", f1_score(np.argmax(y_val_subcat, axis = 1), np.argmax(pred_subcat, axis = 1), average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fe8b7c-66f0-47c0-8bcf-4e27a1f7f676",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
